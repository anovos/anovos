{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "913e627a",
   "metadata": {},
   "source": [
    "# Getting Started with Anovos\n",
    "\n",
    "_Anovos_ provides data scientists and ML engineers with powerful and versatile tools for feature engineering.\n",
    "\n",
    "In this guide, you will learn how to set up get to know key capabilities of _Anovos_."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d75937",
   "metadata": {},
   "source": [
    "## Setting up and verifying the Python and Spark environment\n",
    "\n",
    "_Anovos_ builds on [Apache Spark](https://spark.apache.org/), a highly scalable engine for data engineering. Thus, an installation of Spark is required to run any _Anovos_ code. Further, the Python bindings for Spark (known as `pyspark`) need to be installed in a compatible version.\n",
    "\n",
    "If you are first starting out with _Anovos_ and are not yet familiar with Spark, we recommend you execute this guide through the provided _anovos-demo_ Docker container, which provides a full Spark setup along with compatible versions of Python and a Jupyter notebook environment.\n",
    "\n",
    "**TODO** Add link to how-to for installing Spark and Python\n",
    "\n",
    "**TODO** Add link to Anovos documentation that details how Spark needs to be configured\n",
    "\n",
    "The currently available Beta version of _Anovos_ is specifically built for Spark 2.4.x, Python 3.7.x, and Java 8 (OpenJDK 1.8.x). You can verify that you're running the correct versions by executing the following lines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1df3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c65bd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!spark-submit --version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a259b454",
   "metadata": {},
   "source": [
    "Let's also check that a compatible version of `pyspark` is available within our Python environment: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a8de24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "pyspark.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aac924f",
   "metadata": {},
   "source": [
    "If you haven't done so already, let's install _Anovos_ into the currently active Python environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4acba7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install anovos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f29dd76",
   "metadata": {},
   "source": [
    "As _Anovos_ relies on Spark behind the scenes for most of the heavy lifting, we need to pass an instantiated [`SparkSession`](https://spark.apache.org/docs/2.4.8/api/python/pyspark.sql.html?highlight=sparksession#pyspark.sql.SparkSession) to many of the function calls.\n",
    "\n",
    "**TODO** Add link to Anovos documentation that details how Spark needs to be configured\n",
    "\n",
    "For the purposes of this guide, we'll use the pre-configured `SparkSession` instance provided by [`anovos.shared.spark`](https://github.com/anovos/anovos/blob/main/src/main/anovos/shared/spark.py):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b393b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from anovos.shared.spark import spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c289e4",
   "metadata": {},
   "source": [
    "(Don't worry if this import takes some time and prints a lot of output. You should see that settings are loaded, dependencies are added, and the logger is configured.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f6e50a",
   "metadata": {},
   "source": [
    "## Loading data\n",
    "\n",
    "Data ingestion is the first step in any feature engineering project. _Anovos_ builds on [Spark's data loading capabilites](https://spark.apache.org/docs/2.4.8/api/python/pyspark.sql.html) and can handle different common file formats such as CSV, Parquet, and Avro.\n",
    "\n",
    "_Anovos_' `data_ingest` module provides all data ingestion functionality. It includes functions to merge multiple datasets as well as to select subsets of the loaded data.\n",
    "\n",
    "Let's load the classic [Adult Income dataset](https://archive.ics.uci.edu/ml/datasets/adult) in CSV format, which we'll use throughout this guide:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4b52b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from anovos.data_ingest.data_ingest import read_dataset\n",
    "\n",
    "df = read_dataset(\n",
    "    spark,  # Remember: The first argument of Anovos functions is always an instantiated SparkSession\n",
    "    file_path='../data/income_dataset/csv',\n",
    "    file_type='csv',\n",
    "    file_configs={'header': 'True', 'delimiter': ',', 'inferSchema': 'True'}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbcaa0e7",
   "metadata": {},
   "source": [
    "Note that `df` is a standard Spark [`DataFrame`](https://spark.apache.org/docs/2.4.8/api/python/pyspark.sql.html?highlight=sparksession#pyspark.sql.DataFrame):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d9d365",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe800d8b",
   "metadata": {},
   "source": [
    "Thus, you can use all the built-in methods you might be familiar with, e.g."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7988ed3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26a242f",
   "metadata": {},
   "source": [
    "In this guide, we will work with just a few of the columns in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5031b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from anovos.data_ingest.data_ingest import select_column\n",
    "\n",
    "df = select_column(df, list_of_cols=['age', 'education', 'education-num', 'occupation', 'hours-per-week', 'income'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8418624",
   "metadata": {},
   "source": [
    "## Learning about the data\n",
    "\n",
    "Before we can start to engineer features for, we need to understand the data at hand. For example, we need to verify that the data has sufficient quality or whether we'll have to deal with missing values.\n",
    "\n",
    "_Anovos_' `data_analyzer` module provides three submodules for this purpose:\n",
    "\n",
    "- The functions of `data_analyzer.quality_checker` allow us to detect and fix issues such as empty rows or duplicate entries\n",
    "- `data_analyzer.stats_generator` offers functions to caculate various statistical properties\n",
    "- Finally, `data_analyzer.association_evaluator` enables us to examine a dataset for correlations between columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce67e263",
   "metadata": {},
   "source": [
    "### Assess the data quality\n",
    "\n",
    "Once a new dataset is loaded, its quality should be assessed. Does the dataset contain all the columns we expect? Do we have duplicate values? Did we ingest the expected number of unique data points? _Anovos_' `data_analyzer.quality_checker` module provides convenient functions to answer these questions.\n",
    "\n",
    "For sake of time, in this guide, we'll assume that our dataset does not exhibit such problems. Instead, we'll move on to a more advanced stage of quality assessment and check for outliers in the data. Outliers are data points that deviate significantly from the others. These points can be problematic when training ML models or during inference, as there is very little information about these ranges in the dataset, which in turn leads to a high degree of uncertainty. \n",
    "\n",
    "Outliers in a dataset can be due to rare events. For example, in the case of our income dataset, there might be some individuals that work well beyond retirement age or have worked their way up to a high paying position despite dropping out of high school.\n",
    "\n",
    "However, outliers can also be due to reporting errors. Digits in a number might have been swapped or values might have been given in the wrong order of magnitude. For example, someone might have mistakenly entered a weight in grams instead of kilograms.\n",
    "\n",
    "How we deal with outliers depends on their origin and the application context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca884d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from anovos.data_analyzer.quality_checker import outlier_detection\n",
    "\n",
    "output_df, metric_df = outlier_detection(spark, df, detection_side='both', print_impact=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b113d0e",
   "metadata": {},
   "source": [
    "This tells us that we have little data for older people, which might be an issue later when we are trying to predict the income of this group, so we should keep this in mind.\n",
    "\n",
    "We can ignore the value for `education-num`, a categorical column for which the calculations performed by the `outlier_detection` function are meaningless: In its standard configuration, it checks for values that fulfill at least two of the following criteria: They belong to the smallest or largest 5% of values, they deviate from the mean by more than 3 standard deviations, or they lie below `Q1 - 1.5*IQR` or above `Q3 + 1.5*IQR`, where `Q1` and `Q3` are the first and third quartile, respectively, and `IQR` is the Interquartile Range.\n",
    "\n",
    "**TODO** Add link to docs or code for settings\n",
    "\n",
    "To better understand this, let's examine the hours worked per week reported by the surveyed people:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617e9946",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "px.histogram(df.toPandas(), x='hours-per-week')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35fcb210",
   "metadata": {},
   "source": [
    "We immediately see that the vast majority of people works a standard 40-hour week, while a significant minority reports anywhere between 20 and 60 hours. However, there are also individuals that work very few hours as well as people that reported almost 100 hours per week, more than twice the median. These are the groups detected by the outlier detector.\n",
    "\n",
    "We cannot only detect that there are outliers, we can also deal with them right away. For example, let's remove the rows where individuals reported an excessive amount of hours worked per week:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e361213f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df, metric_df = outlier_detection(spark, df, detection_side='both', list_of_cols=['hours-per-week'], treatment=True, treatment_method='row_removal')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946b867a",
   "metadata": {},
   "source": [
    "Let's check that we have indeed reduced the dataset to entries where the number of hours worked per week lies within a common range:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5503ed57",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(df.toPandas(), x='hours-per-week')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed121d87",
   "metadata": {},
   "source": [
    "### Understand how your data is distributed\n",
    "\n",
    "When familiarizing ourselves with a dataset, one of the first steps is to understand the ranges of values each column contains and how the values are distributed. At the very least, we should learn the minimal and maximal values as well as examine the distribution within that range (e.g., by computing the mean, median, and standard deviation).\n",
    "\n",
    "This information is vital to know. For many popular kinds of ML models, each feature column should be scaled to the same order of magnitude. Further, ML models will generally only work well for ranges of feature values that they were trained on, so we should check that the data points they see during inference lie within the ranges we find.\n",
    "\n",
    "_Anovos_' `data_analyzer.stats_generator` module provides an easy way to quickly calculate a set of common properties for the columns in a dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84be17d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from anovos.data_analyzer.stats_generator import global_summary\n",
    "global_summary(spark, df).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d348964",
   "metadata": {},
   "source": [
    "In most real-world use cases, we are faced with incomplete datasets. For example, customer records might be missing values because we have not had a chance to ask the customer about them. In other cases, there might have been a broken sensor, leading to missing values in a certain time period.\n",
    "\n",
    "In any case, we should know which columns in our dataset might exhibit such issues in order to consider this during feature selection and modeling. We can use the `data_analyzer.stats_generator` module to check this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7c8877",
   "metadata": {},
   "outputs": [],
   "source": [
    "from anovos.data_analyzer.stats_generator import measures_of_counts\n",
    "measures_of_counts(spark, df).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5369e4",
   "metadata": {},
   "source": [
    "There are various ways to deal with missing or unknown values. For example, we could replace missing entries in our dataset with the mean or the median of the respective column. If a column contains mostly null values, it might also be an option to drop it entirely. If a dataset contains a lot of unknown values across all of its columns, it will likely be necessary to design a model that can explicitly handle this situation.\n",
    "\n",
    "Whatever is appropriate in a given scenario, _Anovos_ offers convient functions for this purpose in its `data_transformer` module, which we will have a look at later in this guide."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c7af0e",
   "metadata": {},
   "source": [
    "### Detect correlations within the dataset\n",
    "\n",
    "In machine learning, we are often interested in predicting a class or value (the _label_) from a set of _features_. To decide which features to use in a specific scenario, it is often helpful to determine which columns in a dataset are correlated with the label column. In other words, we would like to find out which columns hold \"predictive power\". \n",
    "\n",
    "**TODO** Check the terminology\n",
    "\n",
    "_Anovos_' `data_analyzer.association_evaluator` provides several functions for this purpose. \n",
    "\n",
    "A commonly used tool is a correlation matrix. It visualizes the degree of pairwise correlation between multiple columns at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6331f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from anovos.data_analyzer.association_evaluator import correlation_matrix\n",
    "correlation_matrix(spark, df, list_of_cols=['age', 'education-num', 'income']).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c706c1f5",
   "metadata": {},
   "source": [
    "From the matrix we see that age and education correlate with income: The older or the higher educated a person, the higher the likelihood that they earn above $50k.\n",
    "\n",
    "**TODO** Add more discussion, point to more advanced functionality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207e5d8a",
   "metadata": {},
   "source": [
    "### Examine Drift\n",
    "\n",
    "Further, we have the drift detection. Drift is a problem for ML models. If over time the distribution changes compared to that of the training, validation, and test data, model performance might degrade.\n",
    "\n",
    "**TODO** Add a link to a conceptual tutorial on Data Drift (e.g., on Medium)\n",
    "\n",
    "_Anovos_ provides an entire module dedicated to detecting various kinds of data drift. We recommend you check how the data you're planning to use evolves over time prior to starting feature engineering.\n",
    "\n",
    "As we only have one dataset, we will artificially create a dataset that has drift by duplicating our dataset and shifting the age column. You could think of this as having surveyed all individuals in the study over their entire life span."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f4882f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "564a3e08",
   "metadata": {},
   "source": [
    "The `drift_detector` helps us ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf417d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "08d18110",
   "metadata": {},
   "source": [
    "We see that ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620982d0",
   "metadata": {},
   "source": [
    "# Transform the data\n",
    "\n",
    "_Key Message(s):_ A basic kind of feature engineering is to apply basic transformations to the data, either to enhance it (e.g., add missing values) or to make it machine-processable (e.g., turn categorical values into cardinal ones)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbcadf69",
   "metadata": {},
   "source": [
    "# Generate a report\n",
    "\n",
    "_Key Message(s):_ Documenting datasets is an important part of data governance; Anovos provides a set of functions to generate extensive data set reports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed5fdaf",
   "metadata": {},
   "source": [
    "## Store the data\n",
    "\n",
    "Once we've prepared and documented the data, it's time to store it so we can use it to train and evaluate ML models.\n",
    "\n",
    "Similar to data ingestion, data storage in _Anovos_ is handled through [Spark's versatile capabilities](). Using the `data_ingest.write_dataset` function, we can write our processed DataFrame to disk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14cee90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from anovos.data_ingest.data_ingest import write_dataset\n",
    "write_dataset(spark, file_path=\".\", file_type=\"csv\", file_configs={\"header\": \"True\", \"delimiter\": \",\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8de4d2f",
   "metadata": {},
   "source": [
    "For now, this final step of data preparation (and this introductory guide) is where _Anovos_' capabilities end. However, over the course of the upcoming releases we will extend _Anovos_ to include adapters for popular AutoML solutions and Feature Stores, allowing you to seamlessly move to model trainin and serving as well as data monitoring."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19aa8b97",
   "metadata": {},
   "source": [
    "## What's next?\n",
    "\n",
    "In this guide, you've had a glimpse at the different capabilities offered by _Anovos_. Of course, we've just scratched the surface and there is much more to see and explore:\n",
    "\n",
    "- The documentation is a great place to get an overview of the available functionality.\n",
    "- To see how different parts of _Anovos_ can be used in practice, have a look at the notebooks our team has prepared for each of the modules.\n",
    "- Finally, to understand how _Anovos_ can be integrated into your Spark ecosystem, see these hints.\n",
    "\n",
    "**TODO** Add links to the respective resources"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}