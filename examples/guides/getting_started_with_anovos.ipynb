{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b2a3aaa",
   "metadata": {},
   "source": [
    "# Getting Started with Anovos\n",
    "\n",
    "_Anovos_ provides data scientists and ML engineers with powerful and versatile tools for feature engineering.\n",
    "\n",
    "In this guide, you will learn how to set up _Anovos_ and get to know key capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1547041f",
   "metadata": {},
   "source": [
    "## Setting up and verifying the Python and Spark environment\n",
    "\n",
    "_Anovos_ builds on [Apache Spark](https://spark.apache.org/), a highly scalable engine for data engineering. Thus, an installation of Spark is required to run any _Anovos_ code. Further, the Python bindings for Spark (known as `pyspark`) need to be installed in a compatible version.\n",
    "\n",
    "If you are first starting out with _Anovos_ and are not yet familiar with Spark, we recommend you execute this guide through the provided _anovos-demo_ Docker container, which provides a full Spark setup along with compatible versions of Python and a Jupyter notebook environment.\n",
    "\n",
    "[Setting up Anovos on Local](https://docs.anovos.ai/using-anovos/setting-up/locally.html)\n",
    "\n",
    "The currently available Beta version of _Anovos_ is specifically built for Spark 2.4.x, Python 3.7.x, and Java 8 (OpenJDK 1.8.x). You can verify that you're running the correct versions by executing the following lines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8678975",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42610c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!spark-submit --version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758fffc3",
   "metadata": {},
   "source": [
    "Let's also check that a compatible version of `pyspark` is available within our Python environment: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aac3cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "pyspark.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34321db0",
   "metadata": {},
   "source": [
    "If you haven't done so already, let's install _Anovos_ into the currently active Python environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71bb35b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install anovos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f41d30e",
   "metadata": {},
   "source": [
    "As _Anovos_ relies on Spark behind the scenes for most of the heavy lifting, we need to pass an instantiated [`SparkSession`](https://spark.apache.org/docs/2.4.8/api/python/pyspark.sql.html?highlight=sparksession#pyspark.sql.SparkSession) to many of the function calls.\n",
    "\n",
    "[Setting up Spark Session for Anovos](https://docs.anovos.ai/using-anovos/setting-up/locally.html)\n",
    "\n",
    "For the purposes of this guide, we'll use the pre-configured `SparkSession` instance provided by [`anovos.shared.spark`](https://github.com/anovos/anovos/blob/main/src/main/anovos/shared/spark.py):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4d2109",
   "metadata": {},
   "outputs": [],
   "source": [
    "from anovos.shared.spark import spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ceacda8",
   "metadata": {},
   "source": [
    "(Don't worry if this import takes some time and prints a lot of output. You should see that settings are loaded, dependencies are added, and the logger is configured.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1de220",
   "metadata": {},
   "source": [
    "## Loading data\n",
    "\n",
    "Data ingestion is the first step in any feature engineering project. _Anovos_ builds on [Spark's data loading capabilites](https://spark.apache.org/docs/2.4.8/api/python/pyspark.sql.html) and can handle different common file formats such as CSV, Parquet, and Avro.\n",
    "\n",
    "_Anovos_' [`data_ingest`](https://docs.anovos.ai/api/data_ingest/data_ingest.html) module provides all data ingestion functionality. It includes functions to merge multiple datasets as well as to select subsets of the loaded data.\n",
    "\n",
    "Let's load the classic [Adult Income dataset](https://archive.ics.uci.edu/ml/datasets/adult) in CSV format, which we'll use throughout this guide:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facc74ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from anovos.data_ingest.data_ingest import read_dataset\n",
    "\n",
    "df = read_dataset(\n",
    "    spark,  # Remember: The first argument of Anovos functions is always an instantiated SparkSession\n",
    "    file_path='../data/income_dataset/csv',\n",
    "    file_type='csv',\n",
    "    file_configs={'header': 'True', 'delimiter': ',', 'inferSchema': 'True'}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54afb10c",
   "metadata": {},
   "source": [
    "Note that `df` is a standard Spark [`DataFrame`](https://spark.apache.org/docs/2.4.8/api/python/pyspark.sql.html?highlight=sparksession#pyspark.sql.DataFrame):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b794142a",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cec30f2",
   "metadata": {},
   "source": [
    "Thus, you can use all the built-in methods you might be familiar with, e.g."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c0cd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ba6969",
   "metadata": {},
   "source": [
    "The [Adult Income dataset's](https://archive.ics.uci.edu/ml/datasets/adult) more than 48k entries each describe a person along with the information whether they earn more than $50k per year.\n",
    "\n",
    "In this guide, we will work with just a few of its columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0f4768",
   "metadata": {},
   "outputs": [],
   "source": [
    "from anovos.data_ingest.data_ingest import select_column\n",
    "\n",
    "df = select_column(df, list_of_cols=['age', 'education', 'education-num', 'occupation', 'hours-per-week', 'income'])\n",
    "df = df.withColumn('income', (df['income'] == '>50K').cast('integer'))  # convert label to integer\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7eed42",
   "metadata": {},
   "source": [
    "## Learning about the data\n",
    "\n",
    "Before we can start to engineer features for, we need to understand the data at hand. For example, we need to verify that the data has sufficient quality or whether we'll have to deal with missing values.\n",
    "\n",
    "_Anovos_' [`data_analyzer`](https://docs.anovos.ai/api/data_analyzer) module provides three submodules for this purpose:\n",
    "\n",
    "- The functions of [`data_analyzer.quality_checker`](https://docs.anovos.ai/api/data_analyzer/quality_checker.html) allow us to detect and fix issues such as empty rows or duplicate entries\n",
    "- [`data_analyzer.stats_generator`](https://docs.anovos.ai/api/data_analyzer/stats_generator.html) offers functions to caculate various statistical properties\n",
    "- Finally, [`data_analyzer.association_evaluator`](https://docs.anovos.ai/api/data_analyzer/association_evaluator.html) enables us to examine a dataset for correlations between columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509fc105",
   "metadata": {},
   "source": [
    "### Assess the data quality\n",
    "\n",
    "Once a new dataset is loaded, its quality should be assessed. Does the dataset contain all the columns we expect? Do we have duplicate values? Did we ingest the expected number of unique data points? _Anovos_' [`data_analyzer.quality_checker`](https://docs.anovos.ai/api/data_analyzer/quality_checker.html) module provides convenient functions to answer these questions.\n",
    "\n",
    "For sake of time, in this guide, we'll assume that our dataset does not exhibit such problems. Instead, we'll move on to a more advanced stage of quality assessment and check for outliers in the data. Outliers are data points that deviate significantly from the others. These points can be problematic when training ML models or during inference, as there is very little information about these ranges in the dataset, which in turn leads to a high degree of uncertainty. \n",
    "\n",
    "Outliers in a dataset can be due to rare events. For example, in the case of our income dataset, there might be some individuals that work well beyond retirement age or have worked their way up to a high paying position despite dropping out of high school.\n",
    "\n",
    "However, outliers can also be due to reporting errors. Digits in a number might have been swapped or values might have been given in the wrong order of magnitude. For example, someone might have mistakenly entered a weight in grams instead of kilograms.\n",
    "\n",
    "How we deal with outliers depends on their origin and the application context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9894484",
   "metadata": {},
   "outputs": [],
   "source": [
    "from anovos.data_analyzer.quality_checker import outlier_detection\n",
    "\n",
    "output_df, metric_df = outlier_detection(spark, df, detection_side='both', print_impact=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae56e2e",
   "metadata": {},
   "source": [
    "This tells us that we have little data for older people, which might be an issue later when we are trying to predict the income of this group, so we should keep this in mind.\n",
    "\n",
    "We can ignore the value for `education-num`, a categorical column for which the calculations performed by the [`outlier_detection`](https://docs.anovos.ai/api/data_analyzer/quality_checker.html) function are meaningless: In its standard configuration, it checks for values that fulfill at least two of the following criteria: They belong to the smallest or largest 5% of values, they deviate from the mean by more than 3 standard deviations, or they lie below `Q1 - 1.5*IQR` or above `Q3 + 1.5*IQR`, where `Q1` and `Q3` are the first and third quartile, respectively, and `IQR` is the [Interquartile Range](https://en.wikipedia.org/wiki/Interquartile_range). (For more details and information on how to utilize additional methods for outlier detection, see [the documentation](https://docs.anovos.ai/api/data_analyzer/quality_checker.html).\n",
    "\n",
    "To better understand this, let's examine the hours worked per week reported by the surveyed people:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70403041",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "px.histogram(df.toPandas(), x='hours-per-week')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c997fd96",
   "metadata": {},
   "source": [
    "We immediately see that the vast majority of people works a standard 40-hour week, while a significant minority reports anywhere between 20 and 60 hours. However, there are also individuals that work very few hours as well as people that reported almost 100 hours per week, more than twice the median. These are the groups detected by the outlier detector.\n",
    "\n",
    "We cannot only detect that there are outliers, we can also deal with them right away. For example, let's remove the rows where individuals reported an excessive amount of hours worked per week:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33388bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df, metric_df = outlier_detection(spark, df, detection_side='both', list_of_cols=['hours-per-week'], treatment=True, treatment_method='row_removal')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5454f2ad",
   "metadata": {},
   "source": [
    "Let's check that we have indeed reduced the dataset to entries where the number of hours worked per week lies within a common range:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a771d480",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(df.toPandas(), x='hours-per-week')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20436d74",
   "metadata": {},
   "source": [
    "### Understand how your data is distributed\n",
    "\n",
    "When familiarizing ourselves with a dataset, one of the first steps is to understand the ranges of values each column contains and how the values are distributed. At the very least, we should learn the minimal and maximal values as well as examine the distribution within that range (e.g., by computing the mean, median, and standard deviation).\n",
    "\n",
    "This information is vital to know. For many popular kinds of ML models, each feature column should be scaled to the same order of magnitude. Further, ML models will generally only work well for ranges of feature values that they were trained on, so we should check that the data points they see during inference lie within the ranges we find.\n",
    "\n",
    "_Anovos_' [`data_analyzer.stats_generator`](https://docs.anovos.ai/api/data_analyzer/stats_generator.html) module provides an easy way to quickly calculate a set of common properties for the columns in a dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ca9a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from anovos.data_analyzer.stats_generator import global_summary\n",
    "\n",
    "global_summary(spark, df).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c207fcc",
   "metadata": {},
   "source": [
    "In most real-world use cases, we are faced with incomplete datasets. For example, customer records might be missing values because we have not had a chance to ask the customer about them. In other cases, there might have been a broken sensor, leading to missing values in a certain time period.\n",
    "\n",
    "In any case, we should know which columns in our dataset might exhibit such issues in order to consider this during feature selection and modeling. We can use the [`data_analyzer.stats_generator`](https://docs.anovos.ai/api/data_analyzer/stats_generator.html) module to check this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146ed755",
   "metadata": {},
   "outputs": [],
   "source": [
    "from anovos.data_analyzer.stats_generator import measures_of_counts\n",
    "\n",
    "measures_of_counts(spark, df).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91576bef",
   "metadata": {},
   "source": [
    "There are various ways to deal with missing or unknown values. For example, we could replace missing entries in our dataset with the mean or the median of the respective column. If a column contains mostly null values, it might also be an option to drop it entirely. If a dataset contains a lot of unknown values across all of its columns, it will likely be necessary to design a model that can explicitly handle this situation.\n",
    "\n",
    "Whatever is appropriate in a given scenario, _Anovos_ offers convient functions for this purpose in its [`data_transformer`](https://docs.anovos.ai/api/data_transformer/transformers.html) module, which we will have a look at later in this guide."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb49a7a4",
   "metadata": {},
   "source": [
    "### Detect correlations within the dataset\n",
    "\n",
    "In machine learning, we are often interested in predicting a class or value (the _label_) from a set of _features_. To decide which features to use in a specific scenario, it is often helpful to determine which columns in a dataset are correlated with the label column. In other words, we would like to find out which columns hold \"predictive power\". \n",
    "\n",
    "_Anovos_' [`data_analyzer.association_evaluator`](https://docs.anovos.ai/api/data_analyzer/association_evaluator.html) provides several functions for this purpose. \n",
    "\n",
    "A commonly used tool is a correlation matrix. It visualizes the degree of pairwise correlation between multiple columns at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5d8a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from anovos.data_analyzer.association_evaluator import correlation_matrix\n",
    "\n",
    "correlation_matrix(spark, df, list_of_cols=['age', 'education-num', 'income']).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9fe2267",
   "metadata": {},
   "source": [
    "From the matrix we see that age and education correlate with income: The older or the higher educated a person, the higher the likelihood that they earn above $50k. However, education and income exhibit a higher degree of correlation than age and income."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f96117f",
   "metadata": {},
   "source": [
    "### Examine Drift\n",
    "\n",
    "Further, we have the drift detection. Drift is a problem for ML models. If over time the distribution changes compared to that of the training, validation, and test data, model performance might degrade. If you're new to this topic, [this introductory blog post](https://towardsdatascience.com/an-introduction-to-machine-learning-engineering-for-production-part-1-2247bbca8a61) provides a first overview.\n",
    "\n",
    "_Anovos_ provides an entire module dedicated to detecting various kinds of data drift. We recommend you check how the data you're planning to use evolves over time prior to starting feature engineering.\n",
    "\n",
    "As we only have one dataset, we will artificially create a dataset that has drift by duplicating our dataset and shifting the age column, artificially aging the population an entire decade:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463d2037",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_shifted = df.withColumn('age', df['age']+10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f172bfd",
   "metadata": {},
   "source": [
    "With the [`drift.detector.statistics`](https://docs.anovos.ai/api/drift/detector.html) function we can compare a given dataset to a baseline. Let's try this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca81462",
   "metadata": {},
   "outputs": [],
   "source": [
    "from anovos.drift.detector import statistics\n",
    "\n",
    "statistics(spark, df_shifted, df).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa6f10e",
   "metadata": {},
   "source": [
    "We see that the drift detector has flagged the `age` column as exhibiting drift. The calculated [Population Stability Index](https://medium.com/model-monitoring-psi/population-stability-index-psi-ab133b0a5d42) signals that the column's value distribution differs significantly from that of the baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb4fceb",
   "metadata": {},
   "source": [
    "## Transform the data\n",
    "\n",
    "Up to this point, we have only analyzed the dataset and removed entire entries based on these analyses. Now it's time to apply changes to the dataset to make it more suitable for future ML model training.\n",
    "\n",
    "Above, we discovered that there are missing values in all five feature columns of the dataset. If we would like to later use an ML model that cannot handle missing values, it might be a sensible option to replace missing values with the feature column's average value.\n",
    "\n",
    "_Anovos_' [`data_transformer`](https://docs.anovos.ai/api/data_transformer/transformers.html) module provides a handy utility function for this and similar transformations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464951dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from anovos.data_transformer.transformers import imputation_MMM\n",
    "\n",
    "transformed_df = imputation_MMM(spark, df, list_of_cols=['age', 'hours-per-week'], method_type='mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e080459e",
   "metadata": {},
   "source": [
    "Let's check that we indeed replaced all missing values in the `age` and `hours-per-week` columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bcc08a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "measures_of_counts(spark, transformed_df).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087588e4",
   "metadata": {},
   "source": [
    "Note that the data transformation capabilites of _Anovos_ are currently limited. Future versions of the library will offer capabilites like auto encoders and methods for dimensionality reduction. For more information, see the [Anovos Product Roadmap](https://docs.anovos.ai/using-anovos/roadmap.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ff5460",
   "metadata": {},
   "source": [
    "# Generate a report\n",
    "\n",
    "Documenting datasets is an important component of any data governance strategy. Thus, _Anovos_ integrates [datapane](https://datapane.com/), a library to create interactive reports.\n",
    "\n",
    "A basic report can be generated with just one line of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c37d840",
   "metadata": {},
   "outputs": [],
   "source": [
    "from anovos.data_report.basic_report_generation import anovos_basic_report\n",
    "\n",
    "anovos_basic_report(spark, transformed_df, output_path='./report')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1f93af",
   "metadata": {},
   "source": [
    "Once the report generation is finished, you can download and view the generated `basic_report.html` stored in [./report](./report/) in any browser.\n",
    "\n",
    "Please note that due to Jupyter's security settings, it is currently not possible to view it directly from within the Jupyter environment spun up by the Docker container.\n",
    "\n",
    "Of course, the format and content of the basic report will likely not be sufficient for your organization's specific needs. Hence, _Anovos_ allows you to conveniently configure and create custom reports using the functions in the [`data_report.report_generation`](https://docs.anovos.ai/api/data_report/report_generation.html) submodule."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09be0ea9",
   "metadata": {},
   "source": [
    "Note: If you are running this notebook via your docker, run the following command in your local terminal to copy the generated report from your docker container to your local system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716009c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "docker cp <containerId>:/anovos/guides/report/basic_report.html /host/path/target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e510db5a",
   "metadata": {},
   "source": [
    "## Store the data\n",
    "\n",
    "Once we've prepared and documented the data, it's time to store it so we can use it to train and evaluate ML models.\n",
    "\n",
    "Similar to data ingestion, data storage in _Anovos_ is handled through Spark's versatile capabilities. Using the `data_ingest.write_dataset` function, we can write our processed DataFrame to disk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d6c09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from anovos.data_ingest.data_ingest import write_dataset\n",
    "\n",
    "write_dataset(transformed_df, file_path=\"./export.csv\", file_type=\"csv\", file_configs={\"header\": \"True\", \"delimiter\": \",\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1151b06",
   "metadata": {},
   "source": [
    "For now, this final step of data preparation (and this introductory guide) is where _Anovos_' capabilities end. However, over the course of the upcoming releases we will extend _Anovos_ to include adapters for popular AutoML solutions and Feature Stores, allowing you to seamlessly move to model trainin and serving as well as data monitoring. For more details and to see what else is ahead, see the [Anovos Product Roadmap](https://docs.anovos.ai/using-anovos/roadmap.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff040bc",
   "metadata": {},
   "source": [
    "## What's next?\n",
    "\n",
    "In this guide, you've had a glimpse at the different capabilities offered by _Anovos_. Of course, we've just scratched the surface and there is much more to see and explore:\n",
    "\n",
    "- The [Anovos documentation](https://docs.anovos.ai/) is a great place to get an overview of the available functionality.\n",
    "- To see how different parts of _Anovos_ can be used in practice, have a look at the [Jupyter notebooks](https://github.com/anovos/anovos/tree/main/examples/notebooks) our team has prepared for each of the modules.\n",
    "- Finally, to understand how _Anovos_ can be integrated into your Spark ecosystem, see these [hints](https://docs.anovos.ai/using-anovos/setting-up/on_aws.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac10572a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
